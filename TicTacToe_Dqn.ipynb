{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TicTacToe_Dqn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw3zP9NuqxF5"
      },
      "source": [
        "# importing  TicTacToe class from environment file\n",
        "import collections\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import deque\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "z3WgiQRlrCMq",
        "outputId": "2e40d8c1-cc09-4f10-8485-626248507ec9"
      },
      "source": [
        "EPISODES = 15000\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.001\n",
        "decay_rate = -0.0003       # epsilon decay rate\n",
        "epsilon = []\n",
        "episd = np.arange(0,EPISODES)\n",
        "for i in range(0,EPISODES):\n",
        "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(decay_rate*i))\n",
        "\n",
        "plt.plot(episd, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcnN3vIAiSsCYQgoCwiGBXEhbbWIq1LXaG12qnLVOu0znTa2uXXcbrMjHWcaavWVqtTdVRcq2htrdZ9BCSssgjGACFhCxCy7/n+/rgHDCEhgdzk3OX9fDzyuGfLvW8O5J3DWc05h4iIRL44vwOIiEhoqNBFRKKECl1EJEqo0EVEooQKXUQkSsT79cHZ2dkuPz/fr48XEYlIK1as2Oucy+lqnm+Fnp+fT1FRkV8fLyISkcxsW3fztMtFRCRKqNBFRKKECl1EJEqo0EVEooQKXUQkSvRY6Gb2kJntMbN13cw3M/u1mRWb2Vozmxn6mCIi0pPebKH/AZh3lPkXABO8rxuB+/oeS0REjlWPhe6cexvYf5RFLgYecUFLgSwzGxmqgJ2tKq3kjr982F9vLyISsUKxD300sL3DeJk37QhmdqOZFZlZUUVFxXF92LryKu5782M+3FV9XN8vIhKtBvSgqHPufudcoXOuMCenyytXezR/2kgCccbi1TtCnE5EJLKFotDLgbwO47netH4xdFASZ52QzeI1O9DTlkREPhGKQl8MXOOd7TILqHLO7QzB+3broumjKKtsYGXpgf78GBGRiNKb0xafAJYAk8yszMyuM7Ovm9nXvUVeBkqAYuAB4OZ+S+s5f8pwkuLjeHGNdruIiBzU490WnXMLe5jvgG+ELFEvpCcn8OkTh/HS2p386PMnER/Q9VEiIhHbhBdNH8Xe2iaWlhztjEoRkdgRsYX+qROHkZ4Uz+I1/Xb8VUQkokRsoScnBDh/ygj+vG4XTa1tfscREfFdxBY6wEWnjKKmsZU3Nx3fRUoiItEkogt9zvihDE1LZLHOdhERiexCjw/EMX/aSP62cTe1Ta1+xxER8VVEFzrAJTNG09jSzssf9Ou1TCIiYS/iC33mmCzGZafx7Ioyv6OIiPgq4gvdzLhs5miWbdnP9v31fscREfFNxBc6BHe7APxxlc5JF5HYFRWFnjs4ldkFQ3luZZnuwCgiMSsqCh3gslNz2bqvnhXbKv2OIiLii6gp9HlTR5CSEODZldrtIiKxKWoKfVBSPBdMHcFLa3fQ2KJbAYhI7ImaQofgbpeaxlZe3bDb7ygiIgMuqgp9VsFQRmYm8+xKnZMuIrEnqgo9EGd8ccZo3t5cwe7qRr/jiIgMqKgqdIArCvNod/CMrhwVkRgTdYU+LjuNM8YN4cnl22lv1znpIhI7oq7QARaePobS/fUsLdnndxQRkQETlYU+b+oIMpLjeWL5dr+jiIgMmKgs9OSEAJfOzOWVdbuorGv2O46IyICIykIHuOq0PJrb2nXDLhGJGVFb6CeNzGB6XhaLlpfqhl0iEhOittABFpyWx+bdtazafsDvKCIi/S6qC/3C6aNITQyw6P1Sv6OIiPS7qC70QUnxXHjyKF5cs5Oaxha/44iI9KuoLnSAL50xhoaWNh0cFZGoF/WFPj0vi5NzM3lkyTYdHBWRqBb1hQ7wlVljKd5TyxJdOSoiUSwmCv3C6aPISk3gf5du8zuKiEi/6VWhm9k8M9tkZsVmdlsX88eY2RtmtsrM1prZ/NBHPX7JCQGuLMzjlfW72VWl2+qKSHTqsdDNLADcC1wATAYWmtnkTov9CHjKOTcDWAD8JtRB++rqM8bS7hyP6xRGEYlSvdlCPx0ods6VOOeagUXAxZ2WcUCGN5wJ7AhdxNAYMzSVuRNzeOL9Uppb2/2OIyIScr0p9NFAx9sWlnnTOroduNrMyoCXgX/o6o3M7EYzKzKzooqKiuOI2zfXzM6noqaJV9bvGvDPFhHpb6E6KLoQ+INzLheYDzxqZke8t3PufudcoXOuMCcnJ0Qf3XvnTMwhb0gKjy7RwVERiT69KfRyIK/DeK43raPrgKcAnHNLgGQgOxQBQykQZ1x9xlje37qfjTur/Y4jIhJSvSn05cAEMxtnZokED3ou7rRMKfAZADM7iWChD/w+lV646rQ8UhICPPTuFr+jiIiEVI+F7pxrBW4BXgE2EjybZb2Z/cTMLvIW+zZwg5mtAZ4AvurC9LLMrNRELj81lxdW76CipsnvOCIiIdOrfejOuZedcxOdc+Odcz/3pv3YObfYG97gnJvjnJvunDvFOffX/gzdV383J5/mtnYe1YVGIhJFYuJK0c4KcgbxmROH8djSbTS2tPkdR0QkJGKy0AGuO3sc++qaeWG17sIoItEhZgt9dsFQThqZwYPvbtFdGEUkKsRsoZsZ1581js27a3nno71+xxER6bOYLXQI3oUxJz2JB3UKo4hEgZgu9MT4OK6ZNZa3NleweXeN33FERPokpgsd4MuzxpKSEOB3b5X4HUVEpE9ivtCHpCWy4PQ8XlhdTvmBBr/jiIgct5gvdIDrzy4A4IG3tZUuIpFLhQ6MzkrhkhmjWbS8lH21uh2AiEQmFbrn6+cW0NjSzsPvbfU7iojIcVGhe04Yls75k4fzh/e2UtvU6nccEZFjpkLv4Ka546lubOWJZXruqIhEHhV6BzPGDGZ2wVB+/24JTa26aZeIRBYVeic3f2o8u6ubeHaFbtolIpFFhd7JWSdkMz0vi3vfKKa5td3vOCIivaZC78TMuPW8CZQfaODZlWV+xxER6TUVehfmTsxhel4W97yurXQRiRwq9C5oK11EIpEKvRsHt9K1L11EIoUKvRsHt9LLKht4TlvpIhIBVOhHcWhfurbSRSQCqNCPouNWuvali0i4U6H3YO7EHGaMyeJXr31EY4uuHhWR8KVC74GZ8b15J7KrupFHlmz1O46ISLdU6L0wq2Ao507M4d43PqaqocXvOCIiXVKh99J3PjeJqoYWPdVIRMKWCr2Xpo7O5MLpo3jw3S3sqWn0O46IyBFU6Mfg25+dSEtbO/e8Xux3FBGRI6jQj0F+dhpXnZbH48tKKd1X73ccEZHDqNCP0Tc/M4H4gHHnXzf5HUVE5DC9KnQzm2dmm8ys2Mxu62aZK81sg5mtN7PHQxszfAzPSOb6swp4cc0OVpZW+h1HROSQHgvdzALAvcAFwGRgoZlN7rTMBOD7wBzn3BTg1n7IGjZumjuenPQkfvbSBpxzfscREQF6t4V+OlDsnCtxzjUDi4CLOy1zA3Cvc64SwDm3J7Qxw0taUjzfOX8SK0sP8OLanX7HEREBelfoo4HtHcbLvGkdTQQmmtn/mdlSM5vX1RuZ2Y1mVmRmRRUVFceXOExcdmouk0dmcMefP9QtAUQkLITqoGg8MAGYCywEHjCzrM4LOefud84VOucKc3JyQvTR/gjEGT/6wkmUH2jgwXe3+B1HRKRXhV4O5HUYz/WmdVQGLHbOtTjntgCbCRZ8VDtzfDafnTyc37xRTEVNk99xRCTG9abQlwMTzGycmSUCC4DFnZZ5nuDWOWaWTXAXTExcI/+D+SfR3NbOXTqNUUR81mOhO+dagVuAV4CNwFPOufVm9hMzu8hb7BVgn5ltAN4AvuOc29dfocPJuOw0rp2dz5NF21lbdsDvOCISw8yv0+4KCwtdUVGRL58dajWNLXz6rrcYlZnMH2+eQ1yc+R1JRKKUma1wzhV2NU9XioZAenICP/r8Sawpq+LJou09f4OISD9QoYfIRdNHcca4Idzxlw/ZX9fsdxwRiUEq9BAxM356yVRqGlu585UP/Y4jIjFIhR5CE4en87U5+Sxavp1Vus+LiAwwFXqIfeu8iQxLT+LHL6ynrV33eRGRgaNCD7FBSfH88POT+aC8ioff2+p3HBGJISr0fnDhySP51KQc/vOvm9i+Xw/CEJGBoULvB2bGz744DYAfPr9Ot9gVkQGhQu8no7NS+O7nJvH25gpeWL3D7zgiEgNU6P3oK7PzmTEmi399cT37anXzLhHpXyr0fhSIM+647GRqm1r52Z82+h1HRKKcCr2fTRyezk1zT+CPq8p548OofpCTiPhMhT4AvvGp8Uwans73nl3LgXrdFkBE+ocKfQAkxQe468rp7K9r5scvrPc7johEKRX6AJk6OpNvfmYCi9fs4E96sLSI9AMV+gC6ee54pudm8qPnP2BPTaPfcUQkyqjQB1B8II67rpxOXXMbP3hOFxyJSGip0AfYCcPS+e7nJvHaxt08vaLM7zgiEkVU6D742pxxzCoYwu2L11NSUet3HBGJEip0H8TFGf991SkkxsfxzUWraG5t9zuSiEQBFbpPRmam8IvLTmZdebWecCQiIaFC99H5U0ZwzeyxPPDOFt7cpKtIRaRvVOg++8H8kzhxRDr//PQancooIn2iQvdZckKAuxfOoLaplW8/tUaPrROR46ZCDwMThqdz+4VTeOejvfz6bx/5HUdEIpQKPUxcdVoel5+ay69f/4g3tD9dRI6DCj1MmBk/vXgqJ47I4NZFq/UsUhE5Zir0MJKSGOC3V8+k3TlufmwljS1tfkcSkQiiQg8zY4em8d9XnsIH5VX864u61a6I9J4KPQydN3k43/jUeJ54fztPvF/qdxwRiRAq9DD1T5+dxLkTc/h/z69jack+v+OISAToVaGb2Twz22RmxWZ221GWu8zMnJkVhi5ibArEGXd/aQZjh6Zy0/+u0EFSEelRj4VuZgHgXuACYDKw0Mwmd7FcOvAtYFmoQ8aqjOQEfn/tabQ7uO7h5dQ0tvgdSUTCWG+20E8Hip1zJc65ZmARcHEXy/0UuAPQ9eshNC47jXu/NJOPK+r4xydX60pSEelWbwp9NLC9w3iZN+0QM5sJ5Dnn/nS0NzKzG82syMyKKioqjjlsrDprQjb/cuFkXtu4h1/ozowi0o0+HxQ1szjgv4Bv97Ssc+5+51yhc64wJyenrx8dU74yayxXzxrD794q4dElW/2OIyJhKL4Xy5QDeR3Gc71pB6UDU4E3zQxgBLDYzC5yzhWFKmisMzNuv3AKOw808i+L1zM8I5nzp4zwO5aIhJHebKEvByaY2TgzSwQWAIsPznTOVTnnsp1z+c65fGApoDLvB/GBOO7+0gym5WbxzUWrWFla6XckEQkjPRa6c64VuAV4BdgIPOWcW29mPzGzi/o7oBwuNTGeB68tZHhGMtc/XMSWvXV+RxKRMGHO+XPWRGFhoSsq0kb88dq6t45L73uPQUnxPHvTmeSkJ/kdSUQGgJmtcM51ea2PrhSNUPnZaTx4bSEVNU185cFlHKhv9juSiPhMhR7BZowZzAPXFFJSUcdX/2c5tU2tfkcSER+p0CPcWROyuedLM/igvIobHi7SLXdFYpgKPQqcP2UEd10xnaVb9nHzYytpaWv3O5KI+ECFHiUumTGan10yldc/3MOtT66mVaUuEnN6c2GRRIgvnzGW+qY2fv7yRnDwywWnkBDQ72yRWKFCjzI3nFOAGfzsTxtpa3f8euEMEuNV6iKxQD/pUej6swv4f1+YzF/W7+Ibj6+kuVW7X0RigQo9Sl131jhuv3Ayr27Yzc2PraCpVWe/iEQ7FXoU++qccfz04im8tnEP1z9cRJ3OUxeJair0KPeV2fn84rKT+b/ivXz598uorNMVpSLRSoUeA648LY/ffPlUNuyo5srfLWFXlR4qJRKNVOgxYt7UEfzha6exs6qRy+57T3dpFIlCKvQYcub4bJ64YRYNLW1cft97rNl+wO9IIhJCKvQYMy03k6e/PpvUpABX3b+Ev6zb5XckEQkRFXoMGp8ziD/ePIeTRmZw02MreODtEvy6L76IhI4KPUZlD0riiRtmMX/qSH7+8kZ+9Pw63f9FJMLp0v8YlpwQ4O6FMxgzNJX73vyY7ZUN3L1wBpkpCX5HE5HjoC30GBcXZ3xv3on8x6XTWPLxXi6+5102767xO5aIHAcVugCw4PQxPHHDLGqb2rjk3v/jzx/s9DuSiBwjFbocUpg/hJf+4SwmDk/npsdWcucrH9LWroOlIpFChS6HGZGZzJN/P4sFp+Vx7xsf87U/LGe/bhcgEhFU6HKEpPgA/37pNH7+xaks+Xgf83/1DstK9vkdS0R6oEKXLpkZXz5jLM/dfCbJCXEsfGApd//tI+2CEQljKnQ5qqmjM3npm2fzhZNHcderm7n2offZU6Obe4mEIxW69GhQUjy/WnAK/3HpNJZv3c/8X73Daxt2+x1LRDpRoUuvmBkLTh/D4lvOIntQEtc/UsT3nllLTWOL39FExKNCl2MyaUQ6L9wyh5vmjufpFdu5QAdMRcKGCl2OWVJ8gO/NO5Gn/n42cWYseGAp//byRhpb9NxSET+p0OW4FeYP4c/fOpsFp43h/rdLmPfLt3mveK/fsURilgpd+iQtKZ5/v3Qaj11/Bg740u+X8d1n1nCgXhcjiQy0XhW6mc0zs01mVmxmt3Ux/5/MbIOZrTWzv5nZ2NBHlXA254RsXrn1HL5+7nieXVnOef/1Fi+t3aH7rIsMoB4L3cwCwL3ABcBkYKGZTe602Cqg0Dl3MvAM8ItQB5Xwl5wQ4LYLTmTxLXMYlZXCLY+v4pqH3qd4T63f0URiQm+20E8Hip1zJc65ZmARcHHHBZxzbzjn6r3RpUBuaGNKJJkyKpPnbjqTf7lwMqu3H2DeL9/m317eqFMcRfpZbwp9NLC9w3iZN6071wF/7mqGmd1oZkVmVlRRUdH7lBJx4gNx/N2ccbzxz3O5bGYuD7xTwqfveotnV5TRrtsHiPSLkB4UNbOrgULgzq7mO+fud84VOucKc3JyQvnREqayByVxx+Un8/zNwd0w3356DZf99j2Wb93vdzSRqNObQi8H8jqM53rTDmNm5wE/BC5yzjWFJp5Ei+l5WfzxpjO58/KT2XGggSt+u4TrHy6ieI+ejiQSKr0p9OXABDMbZ2aJwAJgcccFzGwG8DuCZb4n9DElGsTFGVcU5vHmP3+K73xuEstK9nH+f7/Nbc+uZXe1bvgl0lfWm9PKzGw+8EsgADzknPu5mf0EKHLOLTaz14BpwMHnlpU65y462nsWFha6oqKivqWXiLa/rpm7X/+I/126jUCcce3sfG48p4Chg5L8jiYStsxshXOusMt5fp0nrEKXg0r31fNfr25i8ZodJMUHuGb2WG44p4BsFbvIEVToEhE+rqjlnteLeWF1uYpdpBsqdIkonYv9ysJcrj+7gLwhqX5HE/GdCl0iUklFLb9582NeWF1OW7tj/rSR/P0545mWm+l3NBHfqNAlou2qauR/3tvC40tLqWlqZXbBUG48t4BzJ+QQF2d+xxMZUCp0iQrVjS0ser+Uh97dyq7qRsZlp3H1rLFcfmoumSkJfscTGRAqdIkqza3tvPzBTh5ZspWVpQdISQhwyYxRfGVWPpNHZfgdT6RfqdAlaq0rr+LRJdt4YU05jS3tFI4dzMLTx3DBtBGkJsb7HU8k5FToEvUO1DfzdFEZjy3bxtZ99QxKiucLJ4/kisI8Zo7Jwkz72iU6qNAlZjjneH/Lfp5eUcaf1u6koaWNgpw0rjg1j0tnjmZ4RrLfEUX6RIUuMam2qZWX1+7kqaLtFG2rJM5gVsFQLpw+igumjiArNdHviCLHTIUuMa+kopbnV+/gxTU72LK3jvg445yJOVw4fSSfnTyCQUna3y6RQYUu4nHOsX5HNS+uCZb7jqpGkuLjmDsph/Mnj+DTJw5jcJq23CV8qdBFutDe7lhZWsniNTv46/rd7KpuJBBnnJ4/hPOnDOezk4eTO1i3G5DwokIX6UF7u+OD8ipe3bCbv27YxebdwQdbTx6ZwXknDePcSTlMz80iPhDSh3yJHDMVusgx2rK3jlc37OKv63ezsrSSdgcZyfGcPSGHcyZmc87EHEZmpvgdU2KQCl2kD6rqW3i3eC9vbd7D25v3sst7utKk4emcMzGb2eOHUpg/hIxk3X5A+p8KXSREnHNs3l3LW5v38NbmCpZvqaS5rZ04g2mjM5lVMJRZBUMpzB9Mugpe+oEKXaSfNLa0sbK0kqUl+1n68T5Wba+kpc0dKvgzCoYyc0wWM8cMZpguapIQUKGLDJCG5jZWlVaytGQfS0r2sWZ7Fc1t7QCMzkphhlfuM8cOZvLIDBLjdZBVjs3RCl1XU4iEUEpigDNPyObME7IBaGptY/2OalZuq2RV6QFWbKvkpbXBZ6knxccxbXQmU72vaaMzGZ+TpjNp5LhpC11kgO2samDltgOsLK1k9fYDbNhRTUNLGxAs+ZNGZjB1dAZTRwWLfuLwdG3JyyHa5SISxtraHVv21rKuvJoPyqtYV17Fhh3V1DS1AhAfZxTkpDFxeDqThqczcUTwNW9IKgE9sSnmaJeLSBgLxBknDEvnhGHpXDJjNBC80Kl0fz3rdlSxfkc1H+2uYU3ZgUO7awCSE+KYMCydicPTmTh8EONzBjEuJ428wanaoo9R2kIXiSB1Ta0U76ll0+4aNu+qCb7urmF3ddOhZQJxRt7gFMZlpzEuO1jyBdlpFOSkMTw9Wc9hjXDaQheJEmlJ8UzPy2J6XtZh06vqWyjZW8uWvXVs2VtHyd46tlTUsbRk/6H98xDcqs8bnEru4BTyhqQeMZyREq+HgUQwFbpIFMhMTWDGmMHMGDP4sOnOOXZXN31S9hV1bK+sZ/v+BlZsq6S6sfWw5dOT4skdkkre4BRGD05hZGYyIzK914xkhmUkkRQfGMg/mhwDFbpIFDMzRmQmMyIzmTPHZx8xv6qhhe376ymrbKCssv7Q8NZ9dbxbvJf65rYjvid7UCLDM5IZmZl86HVEZgrD0pPIHpREdnoiQ9OSdMDWByp0kRiWmZJApncefGfOOWqaWtld1cjOqkZ2HXytbmRXVQNllQ0UbavkQH3LEd9rBkPTEoMFPyiJ7EHB4ZxDpR+cNiQtkcGpiSQnaKs/FFToItIlMyMjOYGM5AQmDE/vdrnGljZ2VjVSUdPE3lrvq6aJitpm9tY2UVHTxNZ9deytbaKxpb3L90hOiGNwaiJZqYkMTk3whg9/HZyW4M1PJCM5nvTkBJ3N04kKXUT6JDkh4J1Rk3bU5Zxz1DW3sbdD8VfWt1BZ30xlXTOV9S0cqA++btxVzQFvvP0oJ+IlJ8SRnpxAulfwGcnxZBwa7zicQEZKcHhQUvArNSlAWmI8KQmBqDnzR4UuIgPCzA6VaX4P5X9Qe7ujprE1WPr1zRzwfgHUNLZS3dBCTVMrNY0tVB8cb2xlx4GG4PzGlm7/R9BZamKA1MR47zVAWlJwOC3xk+I/9OrNT0kIkJwQR3JCgOSEgDcenJaSECDJm5YQsAE7c6hXhW5m84BfAQHg9865/+g0Pwl4BDgV2Adc5ZzbGtqoIhJr4uKMzNQEMlMTyKd3vwQ6amlrp6bRK/2GT8q/oaWVuqY26puDrw0tbdQ1tVLf/MlrbVMre6qbqGv+ZHpTa+9+QRz2ZzAOK/ykhDhuPW8iF00fdczv1ZMeC93MAsC9wGeBMmC5mS12zm3osNh1QKVz7gQzWwDcAVwV8rQiIscgIRDHkLTgwddQaG1rp94r/8aWdhqa22hsbaPx4GuHaQ3NbTS1euMtwV8ajS3tNLa2MTi1f+6V35st9NOBYudcCYCZLQIuBjoW+sXA7d7wM8A9ZmbOr8tQRUT6QXwgjoxAXNg+nao3h4hHA9s7jJd507pcxjnXClQBQzu/kZndaGZFZlZUUVFxfIlFRKRLA3rOj3PufudcoXOuMCcnZyA/WkQk6vWm0MuBvA7jud60Lpcxs3ggk+DBURERGSC9KfTlwAQzG2dmicACYHGnZRYD13rDlwOva/+5iMjA6vGgqHOu1cxuAV4heNriQ8659Wb2E6DIObcYeBB41MyKgf0ES19ERAZQr85Dd869DLzcadqPOww3AleENpqIiBwL3QhBRCRKqNBFRKKEb4+gM7MKYNtxfns2sDeEcfpDuGcM93ygjKEQ7vkg/DOGW76xzrkuz/v2rdD7wsyKunumXrgI94zhng+UMRTCPR+Ef8Zwz9eRdrmIiEQJFbqISJSI1EK/3+8AvRDuGcM9HyhjKIR7Pgj/jOGe75CI3IcuIiJHitQtdBER6USFLiISJSKu0M1snpltMrNiM7ttAD83z8zeMLMNZrbezL7lTR9iZq+a2Ufe62BvupnZr72ca81sZof3utZb/iMzu7a7zzzOnAEzW2VmL3nj48xsmZfjSe8Ga5hZkjde7M3P7/Ae3/embzKzz4U4X5aZPWNmH5rZRjObHYbr8B+9v+N1ZvaEmSX7vR7N7CEz22Nm6zpMC9l6M7NTzewD73t+bXZsD8HsJt+d3t/zWjP7o5lldZjX5brp7ue7u/Xf14wd5n3bzJyZZXvjA74OQ8I5FzFfBG8O9jFQACQCa4DJA/TZI4GZ3nA6sBmYDPwCuM2bfhtwhzc8H/gzYMAsYJk3fQhQ4r0O9oYHhzDnPwGPAy95408BC7zh3wI3ecM3A7/1hhcAT3rDk731mgSM89Z3IIT5Hgau94YTgaxwWocEH9ayBUjpsP6+6vd6BM4BZgLrOkwL2XoD3veWNe97LwhBvvOBeG/4jg75ulw3HOXnu7v139eM3vQ8gjcf3AZk+7UOQ/Lvd6A/sE9hYTbwSofx7wPf9ynLCwSfs7oJGOlNGwls8oZ/ByzssPwmb/5C4Hcdph+2XB8z5QJ/Az4NvOT9w9rb4Yfq0Prz/gHP9objveWs8zrtuFwI8mUSLEvrND2c1uHBp28N8dbLS8DnwmE9AvkcXpghWW/evA87TD9suePN12neF4HHvOEu1w3d/Hwf7d9xKDISfGzmdGArnxS6L+uwr1+RtsulN4/D63fef6tnAMuA4c65nd6sXcBwb7i7rP35Z/gl8F3g4KPJhwIHXPCxgJ0/q7vHBvZnvnFABfA/Ftwt9HszSyOM1qFzrhz4T6AU2ElwvawgvNbjQaFab6O94f7M+jWCW63Hk+9o/477xMwuBsqdc2s6zQrHddijSCt035nZIOBZ4FbnXHXHeS74q9mX80DN7AvAHufcCj8+v5fiCf6X9z7n3Aygjrv8iwUAAAJFSURBVOCugkP8XIcA3n7oiwn+8hkFpAHz/MrTW36vt6Mxsx8CrcBjfmfpyMxSgR8AP+5p2UgRaYXem8fh9RszSyBY5o85557zJu82s5He/JHAnh6y9tefYQ5wkZltBRYR3O3yKyDLgo8F7PxZ3T02sD/XcRlQ5pxb5o0/Q7Dgw2UdApwHbHHOVTjnWoDnCK7bcFqPB4VqvZV7wyHPamZfBb4AfNn7pXM8+fbR/frvi/EEf3Gv8X5ucoGVZjbiODL22zo8JgO9j6cvXwS38EoI/iUcPGgyZYA+24BHgF92mn4nhx+Y+oU3/HkOP6jyvjd9CMH9yIO9ry3AkBBnncsnB0Wf5vCDSTd7w9/g8IN5T3nDUzj8gFUJoT0o+g4wyRu+3Vt/YbMOgTOA9UCq97kPA/8QDuuRI/ehh2y9ceQBvfkhyDcP2ADkdFquy3XDUX6+u1v/fc3Yad5WPtmH7ss67PO/34H+wD4HDh593kzwaPgPB/BzzyL4X9q1wGrvaz7B/Xt/Az4CXuvwl2vAvV7OD4DCDu/1NaDY+/q7fsg6l08KvcD7h1bs/VAkedOTvfFib35Bh+//oZd7EyE+Ug+cAhR56/F574cirNYh8K/Ah8A64FGveHxdj8ATBPfptxD8n851oVxvQKH35/0YuIdOB66PM18xwf3NB39eftvTuqGbn+/u1n9fM3aav5VPCn3A12EovnTpv4hIlIi0fegiItINFbqISJRQoYuIRAkVuohIlFChi4hECRW6iEiUUKGLiESJ/w8r43sMlIzt+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Af68Gl4rEDc"
      },
      "source": [
        "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
        "# Important to avoid errors during deployment.\n",
        "def valid_actions(state):\n",
        "    valid_Actions = []\n",
        "    # calling action_space to get all possible actions\n",
        "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
        "    return valid_Actions"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHx55RPrJGR"
      },
      "source": [
        "'''\n",
        "The environment class for the Tic-Tac-Toe problem. Provides methods for initialising a state,\n",
        "checking whether the state is terminal or not,\n",
        "computing new states and rewards given state and action etc.\n",
        "'''\n",
        "\n",
        "class TicTacToe():\n",
        "    def __init__(self):\n",
        "        \"\"\"initialise the board\"\"\"\n",
        "        # initialise state as an array\n",
        "        # initialises the board position, can initialise to an array or matrix\n",
        "        self.reset()\n",
        "        # all possible numbers can initialise to an array or matrix\n",
        "        self.all_possible_numbers = [1,2] \n",
        "    def is_winning(self, curr_state):\n",
        "        \"\"\"Takes state as an input and returns whether any row, column or diagonal has winning sum\"\"\"\n",
        "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
        "        for line in lines:\n",
        "            if np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]):\n",
        "                if (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==1):\n",
        "                    return True\n",
        "                elif (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==2):\n",
        "                    return True\n",
        "        return False\n",
        "    def is_terminal(self, curr_state):\n",
        "        # Terminal state could be winning state or when the board is filled up\n",
        "        if self.is_winning(curr_state) == True:\n",
        "            return True, 'Win'\n",
        "        elif len(self.allowed_positions(curr_state)) ==0:\n",
        "            return True, 'Tie'\n",
        "        else:\n",
        "            return False, 'Resume'\n",
        "    def allowed_positions(self,state):\n",
        "        return np.where(state == 0)[0]\n",
        "    def action_space(self, curr_state):\n",
        "        \"\"\"Takes the current state as input and returns all possible (unused) \n",
        "        values that can be placed on the board\"\"\"\n",
        "        used_values = self.allowed_positions(curr_state)\n",
        "        #used_values = [val for val in curr_state if not np.isnan(val)]\n",
        "        agent_values = [(val,1) for val in used_values]\n",
        "        env_values = [(val,2) for val in used_values]\n",
        "        return [agent_values, env_values]\n",
        "    def state_transition(self, curr_state, curr_action):\n",
        "        \"\"\"Takes current state and action and returns the board position just after agent's move.\"\"\"\n",
        "        position = curr_action[0]\n",
        "        value = curr_action[1]\n",
        "        curr_state[position] = value \n",
        "        return curr_state\n",
        "    def step(self, curr_state, curr_action):\n",
        "        \"\"\"Takes current state and action and returns the next state and reward. \n",
        "        First, check the board position after agent's move, whether the game is won/loss/tied. \n",
        "        Then incorporate environment's move and again check the board status.\"\"\"\n",
        "        terminal_state = False\n",
        "        used_values = np.where(curr_state!=0)[0]\n",
        "        temp_state = self.state_transition(curr_state, curr_action)\n",
        "        if [val for val in used_values if val == curr_action[0]]:\n",
        "            # illegal move \n",
        "            reward = -40\n",
        "            terminal_state = True\n",
        "            game_status = 'Illegal'\n",
        "        else:\n",
        "            terminal_state, game_status = self.is_terminal(temp_state)\n",
        "            if terminal_state == True:\n",
        "                if game_status == 'Win':\n",
        "                    reward=10\n",
        "                else:\n",
        "                    reward=0\n",
        "            else:\n",
        "                position,value = random.choice(self.action_space(temp_state)[1])\n",
        "                temp_state[position]= value\n",
        "                terminal_state, game_status = self.is_terminal(temp_state)\n",
        "                if terminal_state == True:\n",
        "                    if game_status == 'Win':\n",
        "                        reward=-10\n",
        "                        game_status = 'Loose'\n",
        "                    else:\n",
        "                        reward=0\n",
        "                else:\n",
        "                    reward=-1\n",
        "        return temp_state, reward, terminal_state,game_status\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(9,dtype=int)\n",
        "        return self.state"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX9GkaQ4rWwk"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01 \n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = -0.0003\n",
        "        self.epsilon = 1\n",
        "        \n",
        "        self.batch_size = 256\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=512)\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets       \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        #model.summary()\n",
        "        return model\n",
        "    \n",
        "    def encode(self,state):\n",
        "        en = en = np.zeros([9,3],dtype=int)\n",
        "        for i,l in enumerate(state):\n",
        "            en[i][l] = 1\n",
        "        return en.reshape(1,27)\n",
        "    \n",
        "    def update_epsilon(self,episode):\n",
        "        self.epsilon = (self.epsilon_max - self.epsilon_min) * np.exp(self.epsilon_decay * episode)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # get action from model using epsilon-greedy policy\n",
        "        # Decay in ε after we generate each sample from the environment\n",
        "        possible_actions = valid_actions(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(possible_actions)\n",
        "        else :\n",
        "            q_value = self.model.predict(self.encode(state))\n",
        "            # truncate the array to only those actions that are part of the ride  requests.\n",
        "            return (np.argmax(q_value),1)\n",
        "        #self.epsilon = (self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*i))\n",
        "\n",
        "    def append_sample(self, curr_state, action, reward, next_state,done):\n",
        "        # save sample <s,a,r,s'> to the replay memory\n",
        "        #print(curr_state, action, reward, next_state,done)\n",
        "        self.memory.append((curr_state, action, reward, next_state, done))\n",
        "    \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            # Sample batch from the memory\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            \n",
        "            actions, rewards, done = [], [], []\n",
        "            \n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_bool = mini_batch[i]\n",
        "                update_input[i] = self.encode(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                done.append(done_bool)\n",
        "                update_output[i] = self.encode(next_state)\n",
        "                \n",
        "            # 1. Predict the target from earlier model\n",
        "            target = self.model.predict(update_input)    \n",
        "            # 2. Get the target for the Q-network\n",
        "            target_qval = self.model.predict(update_output)    \n",
        "                \n",
        "            #3. Update your 'update_output' and 'update_input' batch\n",
        "            for i in range(self.batch_size):\n",
        "                if not done[i]:\n",
        "                    # Only take the max q_value from valid actions from next state\n",
        "                    target[i][actions[i][0]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "                else:\n",
        "                    target[i][actions[i][0]] = rewards[i]\n",
        "                \n",
        "            # 4. Fit your model and track the loss values\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miXsU2ourc6h",
        "outputId": "4f69a540-4dca-465a-bb37-17d8221acc5d"
      },
      "source": [
        "Episodes = 15000\n",
        "start_time = time.time()\n",
        "episode_time= time.time()\n",
        "agent_won = 0\n",
        "envn_won = 0\n",
        "tie_cnt = 0\n",
        "chk_prt_epsd = 500\n",
        "env = TicTacToe()\n",
        "agent = DQNAgent(3*3*3,3*3)\n",
        "#agent.model.load_weights('model12')\n",
        "score = 0\n",
        "status = 'Tie'\n",
        "ilcnt = 0\n",
        "wincnt = 0 \n",
        "locnt = 0\n",
        "ticnt = 0\n",
        "dfcnt = 0\n",
        "\n",
        "for episode in range(Episodes):\n",
        "    curr_state = []\n",
        "    curr_state = env.reset()\n",
        "    terminal_state = False\n",
        "    while not terminal_state:\n",
        "        state = curr_state.copy()\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward,terminal_state,status = env.step(state, action)\n",
        "        agent.append_sample(curr_state,action,reward,next_state,terminal_state)\n",
        "        curr_state = next_state\n",
        "        score += reward\n",
        "        agent.train_model()\n",
        "        \n",
        "    if status == 'Illegal':\n",
        "        ilcnt += 1;\n",
        "    elif status == 'Win':\n",
        "        wincnt += 1\n",
        "    elif status == 'Loose':\n",
        "        locnt += 1\n",
        "    elif status == 'Tie':\n",
        "        ticnt += 1\n",
        "    else:\n",
        "        dfcnt += 1\n",
        "    agent.update_epsilon(episode)\n",
        "    if (episode % chk_prt_epsd) == 0:\n",
        "        print(f'Episode: {episode} Epsilon: {agent.epsilon:.2f} Reward: {score} ElTime:{time.time() - episode_time:.2f}')\n",
        "        print(f'win:{wincnt} Lost:{locnt} Tie:{ticnt} Illegal:{ilcnt} Dlf:{dfcnt}')\n",
        "        score = 0\n",
        "        status = 'Tie'\n",
        "        ilcnt = 0\n",
        "        wincnt = 0 \n",
        "        locnt = 0\n",
        "        ticnt = 0\n",
        "        dfcnt = 0\n",
        "        episode_time = time.time()\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Epsilon: 1.00 Reward: -4 ElTime:0.03\n",
            "win:0 Lost:0 Tie:1 Illegal:0 Dlf:0\n",
            "Episode: 500 Epsilon: 0.86 Reward: -2409 ElTime:214.63\n",
            "win:263 Lost:118 Tie:62 Illegal:57 Dlf:0\n",
            "Episode: 1000 Epsilon: 0.74 Reward: -6631 ElTime:221.99\n",
            "win:205 Lost:111 Tie:31 Illegal:153 Dlf:0\n",
            "Episode: 1500 Epsilon: 0.64 Reward: -10773 ElTime:215.02\n",
            "win:155 Lost:64 Tie:22 Illegal:259 Dlf:0\n",
            "Episode: 2000 Epsilon: 0.55 Reward: -10243 ElTime:214.68\n",
            "win:166 Lost:72 Tie:15 Illegal:247 Dlf:0\n",
            "Episode: 2500 Epsilon: 0.47 Reward: -7714 ElTime:237.04\n",
            "win:210 Lost:78 Tie:22 Illegal:190 Dlf:0\n",
            "Episode: 3000 Epsilon: 0.41 Reward: -7787 ElTime:229.69\n",
            "win:201 Lost:83 Tie:26 Illegal:190 Dlf:0\n",
            "Episode: 3500 Epsilon: 0.35 Reward: -9514 ElTime:233.58\n",
            "win:184 Lost:76 Tie:9 Illegal:231 Dlf:0\n",
            "Episode: 4000 Epsilon: 0.30 Reward: -16464 ElTime:187.84\n",
            "win:65 Lost:37 Tie:4 Illegal:394 Dlf:0\n",
            "Episode: 4500 Epsilon: 0.26 Reward: -13635 ElTime:210.73\n",
            "win:128 Lost:35 Tie:1 Illegal:336 Dlf:0\n",
            "Episode: 5000 Epsilon: 0.22 Reward: -9124 ElTime:220.11\n",
            "win:213 Lost:39 Tie:6 Illegal:242 Dlf:0\n",
            "Episode: 5500 Epsilon: 0.19 Reward: -8194 ElTime:219.97\n",
            "win:246 Lost:21 Tie:1 Illegal:232 Dlf:0\n",
            "Episode: 6000 Epsilon: 0.17 Reward: -7381 ElTime:216.73\n",
            "win:262 Lost:20 Tie:1 Illegal:217 Dlf:0\n",
            "Episode: 6500 Epsilon: 0.14 Reward: -5634 ElTime:224.62\n",
            "win:293 Lost:29 Tie:1 Illegal:177 Dlf:0\n",
            "Episode: 7000 Epsilon: 0.12 Reward: -5534 ElTime:218.98\n",
            "win:298 Lost:23 Tie:1 Illegal:178 Dlf:0\n",
            "Episode: 7500 Epsilon: 0.11 Reward: -3486 ElTime:217.51\n",
            "win:331 Lost:34 Tie:2 Illegal:133 Dlf:0\n",
            "Episode: 8000 Epsilon: 0.09 Reward: -3679 ElTime:221.07\n",
            "win:332 Lost:27 Tie:2 Illegal:139 Dlf:0\n",
            "Episode: 8500 Epsilon: 0.08 Reward: -2859 ElTime:223.14\n",
            "win:347 Lost:32 Tie:0 Illegal:121 Dlf:0\n",
            "Episode: 9000 Epsilon: 0.07 Reward: -2888 ElTime:220.67\n",
            "win:337 Lost:46 Tie:1 Illegal:116 Dlf:0\n",
            "Episode: 9500 Epsilon: 0.06 Reward: -2515 ElTime:220.07\n",
            "win:357 Lost:23 Tie:2 Illegal:118 Dlf:0\n",
            "Episode: 10000 Epsilon: 0.05 Reward: -1535 ElTime:237.04\n",
            "win:373 Lost:31 Tie:1 Illegal:95 Dlf:0\n",
            "Episode: 10500 Epsilon: 0.04 Reward: -1225 ElTime:226.95\n",
            "win:388 Lost:16 Tie:0 Illegal:96 Dlf:0\n",
            "Episode: 11000 Epsilon: 0.04 Reward: -556 ElTime:237.91\n",
            "win:390 Lost:35 Tie:1 Illegal:74 Dlf:0\n",
            "Episode: 11500 Epsilon: 0.03 Reward: 542 ElTime:238.37\n",
            "win:415 Lost:31 Tie:0 Illegal:54 Dlf:0\n",
            "Episode: 12000 Epsilon: 0.03 Reward: -555 ElTime:227.94\n",
            "win:393 Lost:29 Tie:1 Illegal:77 Dlf:0\n",
            "Episode: 12500 Epsilon: 0.02 Reward: 200 ElTime:223.15\n",
            "win:420 Lost:11 Tie:0 Illegal:69 Dlf:0\n",
            "Episode: 13000 Epsilon: 0.02 Reward: -478 ElTime:223.00\n",
            "win:405 Lost:13 Tie:0 Illegal:82 Dlf:0\n",
            "Episode: 13500 Epsilon: 0.02 Reward: 832 ElTime:224.12\n",
            "win:427 Lost:20 Tie:0 Illegal:53 Dlf:0\n",
            "Episode: 14000 Epsilon: 0.01 Reward: 398 ElTime:219.68\n",
            "win:417 Lost:22 Tie:0 Illegal:61 Dlf:0\n",
            "Episode: 14500 Epsilon: 0.01 Reward: 405 ElTime:224.89\n",
            "win:418 Lost:22 Tie:0 Illegal:60 Dlf:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1hfqwSkBK1K"
      },
      "source": [
        "agent.model.save_weights('model12')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHrVHwKbBW59"
      },
      "source": [
        "agent.model.load_weights('model12')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}