{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TicTacToe_Dqn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw3zP9NuqxF5"
      },
      "source": [
        "# importing  TicTacToe class from environment file\n",
        "import collections\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import deque\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "z3WgiQRlrCMq",
        "outputId": "3e2f8f3e-e303-418f-bce7-7fc505b0b3b2"
      },
      "source": [
        "EPISODES = 5000\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "decay_rate = -0.0005       # epsilon decay rate\n",
        "epsilon = []\n",
        "episd = np.arange(0,EPISODES)\n",
        "for i in range(0,EPISODES):\n",
        "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(decay_rate*i))\n",
        "\n",
        "plt.plot(episd, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Af68Gl4rEDc"
      },
      "source": [
        "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
        "# Important to avoid errors during deployment.\n",
        "def valid_actions(state):\n",
        "    valid_Actions = []\n",
        "    # calling action_space to get all possible actions\n",
        "    valid_Actions = [i for i in env.action_space(state)[0]]\n",
        "    return valid_Actions"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHx55RPrJGR"
      },
      "source": [
        "'''\n",
        "The environment class for the Tic-Tac-Toe problem. Provides methods for initialising a state,\n",
        "checking whether the state is terminal or not,\n",
        "computing new states and rewards given state and action etc.\n",
        "'''\n",
        "\n",
        "class TicTacToe():\n",
        "    def __init__(self):\n",
        "        \"\"\"initialise the board\"\"\"\n",
        "        # initialise state as an array\n",
        "        # initialises the board position, can initialise to an array or matrix\n",
        "        self.reset()\n",
        "        # all possible numbers can initialise to an array or matrix\n",
        "        self.all_possible_numbers = [1,2] \n",
        "    def is_winning(self, curr_state):\n",
        "        \"\"\"Takes state as an input and returns whether any row, column or diagonal has winning sum\"\"\"\n",
        "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
        "        for line in lines:\n",
        "            if np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]):\n",
        "                if (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==1):\n",
        "                    return True\n",
        "                elif (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==2):\n",
        "                    return True\n",
        "        return False\n",
        "    def is_terminal(self, curr_state):\n",
        "        # Terminal state could be winning state or when the board is filled up\n",
        "        if self.is_winning(curr_state) == True:\n",
        "            return True, 'Win'\n",
        "        elif len(self.allowed_positions(curr_state)) ==0:\n",
        "            return True, 'Tie'\n",
        "        else:\n",
        "            return False, 'Resume'\n",
        "    def allowed_positions(self,state):\n",
        "        return np.where(state == 0)[0]\n",
        "    def action_space(self, curr_state):\n",
        "        \"\"\"Takes the current state as input and returns all possible (unused) \n",
        "        values that can be placed on the board\"\"\"\n",
        "        used_values = self.allowed_positions(curr_state)\n",
        "        #used_values = [val for val in curr_state if not np.isnan(val)]\n",
        "        agent_values = [(val,1) for val in used_values]\n",
        "        env_values = [(val,2) for val in used_values]\n",
        "        return [agent_values, env_values]\n",
        "    def state_transition(self, curr_state, curr_action):\n",
        "        \"\"\"Takes current state and action and returns the board position just after agent's move.\"\"\"\n",
        "        position = curr_action[0]\n",
        "        value = curr_action[1]\n",
        "        curr_state[position] = value \n",
        "        return curr_state\n",
        "    def step(self, curr_state, curr_action):\n",
        "        \"\"\"Takes current state and action and returns the next state and reward. \n",
        "        First, check the board position after agent's move, whether the game is won/loss/tied. \n",
        "        Then incorporate environment's move and again check the board status.\"\"\"\n",
        "        terminal_state = False\n",
        "        used_values = np.where(curr_state!=0)[0]\n",
        "        temp_state = self.state_transition(curr_state, curr_action)\n",
        "        if [val for val in used_values if val == curr_action[0]]:\n",
        "            # illegal move \n",
        "            reward = -40\n",
        "            terminal_state = True\n",
        "            game_status = 'Illegal'\n",
        "        else:\n",
        "            terminal_state, game_status = self.is_terminal(temp_state)\n",
        "            if terminal_state == True:\n",
        "                if game_status == 'Win':\n",
        "                    reward=10\n",
        "                else:\n",
        "                    reward=0\n",
        "            else:\n",
        "                position,value = random.choice(self.action_space(temp_state)[1])\n",
        "                temp_state[position]= value\n",
        "                terminal_state, game_status = self.is_terminal(temp_state)\n",
        "                if terminal_state == True:\n",
        "                    if game_status == 'Win':\n",
        "                        reward=-10\n",
        "                        game_status = 'Loose'\n",
        "                    else:\n",
        "                        reward=0\n",
        "                else:\n",
        "                    reward=-1\n",
        "        return temp_state, reward, terminal_state,game_status\n",
        "    def reset(self):\n",
        "        self.state = np.zeros(9,dtype=int)\n",
        "        return self.state"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX9GkaQ4rWwk"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01 \n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = -0.0005\n",
        "        self.epsilon = 1\n",
        "        \n",
        "        self.batch_size = 128\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=1024)\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets       \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
        "        #model.summary()\n",
        "        return model\n",
        "    \n",
        "    def encode(self,state):\n",
        "        en = en = np.zeros([9,3],dtype=int)\n",
        "        for i,l in enumerate(state):\n",
        "            en[i][l] = 1\n",
        "        return en.reshape(1,27)\n",
        "    \n",
        "    def update_epsilon(self,episode):\n",
        "        self.epsilon = (self.epsilon_max - self.epsilon_min) * np.exp(self.epsilon_decay * episode)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # get action from model using epsilon-greedy policy\n",
        "        # Decay in Îµ after we generate each sample from the environment\n",
        "        possible_actions = valid_actions(state)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(possible_actions)\n",
        "        else :\n",
        "            q_value = self.model.predict(self.encode(state))\n",
        "            # truncate the array to only those actions that are part of the ride  requests.\n",
        "            return (np.argmax(q_value),1)\n",
        "        #self.epsilon = (self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*i))\n",
        "\n",
        "    def append_sample(self, curr_state, action, reward, next_state,done):\n",
        "        # save sample <s,a,r,s'> to the replay memory\n",
        "        #print(curr_state, action, reward, next_state,done)\n",
        "        self.memory.append((curr_state, action, reward, next_state, done))\n",
        "    \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            # Sample batch from the memory\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            \n",
        "            actions, rewards, done = [], [], []\n",
        "            \n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_bool = mini_batch[i]\n",
        "                update_input[i] = self.encode(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                done.append(done_bool)\n",
        "                update_output[i] = self.encode(next_state)\n",
        "                \n",
        "            # 1. Predict the target from earlier model\n",
        "            target = self.model.predict(update_input)    \n",
        "            # 2. Get the target for the Q-network\n",
        "            target_qval = self.model.predict(update_output)    \n",
        "                \n",
        "            #3. Update your 'update_output' and 'update_input' batch\n",
        "            for i in range(self.batch_size):\n",
        "                if not done[i]:\n",
        "                    # Only take the max q_value from valid actions from next state\n",
        "                    target[i][actions[i][0]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "                else:\n",
        "                    target[i][actions[i][0]] = rewards[i]\n",
        "                \n",
        "            # 4. Fit your model and track the loss values\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miXsU2ourc6h",
        "outputId": "f8cfd49c-d362-4b41-844b-e89bebad4129"
      },
      "source": [
        "Episodes = 5000\n",
        "start_time = time.time()\n",
        "episode_time= time.time()\n",
        "agent_won = 0\n",
        "envn_won = 0\n",
        "tie_cnt = 0\n",
        "chk_prt_epsd = 500\n",
        "env = TicTacToe()\n",
        "agent = DQNAgent(3*3*3,3*3)\n",
        "score = 0\n",
        "status = 'Tie'\n",
        "ilcnt = 0\n",
        "wincnt = 0 \n",
        "locnt = 0\n",
        "ticnt = 0\n",
        "dfcnt = 0\n",
        "\n",
        "for episode in range(Episodes):\n",
        "    curr_state = []\n",
        "    curr_state = env.reset()\n",
        "    terminal_state = False\n",
        "    while not terminal_state:\n",
        "        state = curr_state.copy()\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward,terminal_state,status = env.step(state, action)\n",
        "        agent.append_sample(curr_state,action,reward,next_state,terminal_state)\n",
        "        curr_state = next_state\n",
        "        score += reward\n",
        "        agent.train_model()\n",
        "        \n",
        "    if status == 'Illegal':\n",
        "        ilcnt += 1;\n",
        "    elif status == 'Win':\n",
        "        wincnt += 1\n",
        "    elif status == 'Loose':\n",
        "        locnt += 1\n",
        "    elif status == 'Tie':\n",
        "        ticnt += 1\n",
        "    else:\n",
        "        dfcnt += 1\n",
        "    agent.update_epsilon(episode)\n",
        "    if (episode % chk_prt_epsd) == 0:\n",
        "        print(f'Episode: {episode} Epsilon: {agent.epsilon:.2f} Reward: {score} ElTime:{time.time() - episode_time:.2f}')\n",
        "        print(f'win:{wincnt} Lost:{locnt} Tie:{ticnt} Illegal:{ilcnt} Dlf:{dfcnt}')\n",
        "        score = 0\n",
        "        status = 'Tie'\n",
        "        ilcnt = 0\n",
        "        wincnt = 0 \n",
        "        locnt = 0\n",
        "        ticnt = 0\n",
        "        dfcnt = 0\n",
        "        episode_time = time.time()\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Epsilon: 0.99 Reward: 6 ElTime:0.02\n",
            "win:1 Lost:0 Tie:0 Illegal:0 Dlf:0\n",
            "Episode: 500 Epsilon: 0.77 Reward: -2809 ElTime:207.73\n",
            "win:251 Lost:144 Tie:46 Illegal:59 Dlf:0\n",
            "Episode: 1000 Epsilon: 0.60 Reward: -2652 ElTime:221.82\n",
            "win:272 Lost:121 Tie:41 Illegal:66 Dlf:0\n",
            "Episode: 1500 Epsilon: 0.47 Reward: -3935 ElTime:232.21\n",
            "win:253 Lost:106 Tie:44 Illegal:97 Dlf:0\n",
            "Episode: 2000 Epsilon: 0.36 Reward: -3936 ElTime:234.48\n",
            "win:261 Lost:95 Tie:42 Illegal:102 Dlf:0\n",
            "Episode: 2500 Epsilon: 0.28 Reward: -4659 ElTime:233.22\n",
            "win:261 Lost:83 Tie:31 Illegal:125 Dlf:0\n",
            "Episode: 3000 Epsilon: 0.22 Reward: -5780 ElTime:233.14\n",
            "win:236 Lost:94 Tie:25 Illegal:145 Dlf:0\n",
            "Episode: 3500 Epsilon: 0.17 Reward: -5265 ElTime:237.21\n",
            "win:249 Lost:78 Tie:34 Illegal:139 Dlf:0\n",
            "Episode: 4000 Epsilon: 0.13 Reward: -4239 ElTime:240.69\n",
            "win:282 Lost:75 Tie:20 Illegal:123 Dlf:0\n",
            "Episode: 4500 Epsilon: 0.10 Reward: -3678 ElTime:240.07\n",
            "win:299 Lost:74 Tie:14 Illegal:113 Dlf:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1hfqwSkBK1K"
      },
      "source": [
        "agent.model.save_weights('model12')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHrVHwKbBW59"
      },
      "source": [
        "agent.model.load_weights('model12')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}