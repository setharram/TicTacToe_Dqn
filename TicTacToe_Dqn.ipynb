{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Nw3zP9NuqxF5"
   },
   "outputs": [],
   "source": [
    "# importing  TicTacToe class from environment file\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0UHx55RPrJGR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The environment class for the Tic-Tac-Toe problem. Provides methods for initialising a state,\n",
    "checking whether the state is terminal or not,\n",
    "computing new states and rewards given state and action etc.\n",
    "'''\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self,grid_size):\n",
    "        \"\"\"initialise the board\"\"\"\n",
    "        # initialise state as an array\n",
    "        # initialises the board position, can initialise to an array or matrix\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "        # all possible numbers can initialise to an array or matrix\n",
    "    def is_winning(self, curr_state):\n",
    "        \"\"\"Takes state as an input and returns whether any row, column or diagonal has winning sum\"\"\"\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        for line in lines:\n",
    "            if np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]):\n",
    "                if (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==1):\n",
    "                    return True\n",
    "                elif (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==2):\n",
    "                    return True\n",
    "        return False\n",
    "    def is_terminal(self, curr_state):\n",
    "        # Terminal state could be winning state or when the board is filled up\n",
    "        if self.is_winning(curr_state) == True:\n",
    "            return True, 'Won'\n",
    "        elif len(self.allowed_positions(curr_state)) ==0:\n",
    "            # all space got filled\n",
    "            return True, 'Tie'\n",
    "        else:\n",
    "            return False, 'Resume'\n",
    "    def allowed_positions(self,state):\n",
    "        # zero represent state available\n",
    "        return np.where(state == 0)[0]\n",
    "    def valid_actions(self, curr_state,player):\n",
    "        \"\"\"Takes the current state as input and returns all possible (unused) \n",
    "        values that can be placed on the board\"\"\"\n",
    "        values = []\n",
    "        unused_values = self.allowed_positions(curr_state)\n",
    "        if player == \"agent\":\n",
    "            values = [(val,1) for val in unused_values]\n",
    "        elif player ==\"Env\":\n",
    "            values = [(val,2) for val in unused_values]\n",
    "        return values\n",
    "    def state_transition(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the board position just after agent's move.\"\"\"\n",
    "        position = curr_action[0]\n",
    "        value = curr_action[1]\n",
    "        curr_state[position] = value \n",
    "        return curr_state\n",
    "    def step(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the next state and reward. \n",
    "        First, check the board position after agent's move, whether the game is won/loss/tied. \n",
    "        Then incorporate environment's move and again check the board status.\"\"\"\n",
    "        terminal_state = False\n",
    "        used_values = np.where(curr_state!=0)[0]\n",
    "        temp_state = self.state_transition(curr_state, curr_action)\n",
    "        if [val for val in used_values if val == curr_action[0]]:\n",
    "            # illegal move \n",
    "            reward = -40\n",
    "            terminal_state = True\n",
    "            game_status = 'Illegal'\n",
    "        else:\n",
    "            terminal_state, game_status = self.is_terminal(temp_state)\n",
    "            if terminal_state == True:\n",
    "                if game_status == 'Won':\n",
    "                    reward=10\n",
    "                else:\n",
    "                    reward=0\n",
    "            else:\n",
    "                # make random move from Environment\n",
    "                position,value = random.choice(self.valid_actions(temp_state,\"Env\"))\n",
    "                temp_state[position]= value\n",
    "                terminal_state, game_status = self.is_terminal(temp_state)\n",
    "                if terminal_state == True:\n",
    "                    if game_status == 'Won':\n",
    "                        reward=-10\n",
    "                        game_status = 'Lose'\n",
    "                    else:\n",
    "                        reward=0\n",
    "                else:\n",
    "                    reward=-1\n",
    "        return temp_state, reward, terminal_state,game_status\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.grid_size,dtype=int)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPISODES = 15000\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "decay_rate = -0.0003       # epsilon decay rate\n",
    "epsilon = []\n",
    "episd = np.arange(0,EPISODES)\n",
    "for i in range(0,EPISODES):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(decay_rate*i))\n",
    "\n",
    "plt.plot(episd, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FX9GkaQ4rWwk"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01 \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = -0.0003\n",
    "        self.epsilon = 1\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=512)\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    def encode(self,state):\n",
    "        en = en = np.zeros([9,3],dtype=int)\n",
    "        for i,l in enumerate(state):\n",
    "            en[i][l] = 1\n",
    "        return en.reshape(1,27)\n",
    "    \n",
    "    def update_epsilon(self,episode):\n",
    "        self.epsilon = (self.epsilon_max - self.epsilon_min) * np.exp(self.epsilon_decay * episode)\n",
    "\n",
    "    def get_action(self, state,train=True):\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after we generate each sample from the environment\n",
    "        possible_actions = env.valid_actions(state,\"agent\")\n",
    "        if np.random.rand() <= self.epsilon and train==True:\n",
    "            return random.choice(possible_actions)\n",
    "        else :\n",
    "            q_value = self.model.predict(self.encode(state))\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            return (np.argmax(q_value),1)\n",
    "        #self.epsilon = (self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay*i))\n",
    "\n",
    "    def append_sample(self, curr_state, action, reward, next_state,done):\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        #print(curr_state, action, reward, next_state,done)\n",
    "        self.memory.append((curr_state, action, reward, next_state, done))\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_bool = mini_batch[i]\n",
    "                update_input[i] = self.encode(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_bool)\n",
    "                update_output[i] = self.encode(next_state)\n",
    "                \n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)    \n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)    \n",
    "                \n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                if not done[i]:\n",
    "                    # Only take the max q_value from valid actions from next state\n",
    "                    target[i][actions[i][0]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                else:\n",
    "                    target[i][actions[i][0]] = rewards[i]\n",
    "                \n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miXsU2ourc6h",
    "outputId": "4f69a540-4dca-465a-bb37-17d8221acc5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epsilon: 1.00 Reward: -4 ElTime:0.03\n",
      "win:0 Lost:0 Tie:1 Illegal:0 Dlf:0\n",
      "Episode: 500 Epsilon: 0.86 Reward: -2409 ElTime:214.63\n",
      "win:263 Lost:118 Tie:62 Illegal:57 Dlf:0\n",
      "Episode: 1000 Epsilon: 0.74 Reward: -6631 ElTime:221.99\n",
      "win:205 Lost:111 Tie:31 Illegal:153 Dlf:0\n",
      "Episode: 1500 Epsilon: 0.64 Reward: -10773 ElTime:215.02\n",
      "win:155 Lost:64 Tie:22 Illegal:259 Dlf:0\n",
      "Episode: 2000 Epsilon: 0.55 Reward: -10243 ElTime:214.68\n",
      "win:166 Lost:72 Tie:15 Illegal:247 Dlf:0\n",
      "Episode: 2500 Epsilon: 0.47 Reward: -7714 ElTime:237.04\n",
      "win:210 Lost:78 Tie:22 Illegal:190 Dlf:0\n",
      "Episode: 3000 Epsilon: 0.41 Reward: -7787 ElTime:229.69\n",
      "win:201 Lost:83 Tie:26 Illegal:190 Dlf:0\n",
      "Episode: 3500 Epsilon: 0.35 Reward: -9514 ElTime:233.58\n",
      "win:184 Lost:76 Tie:9 Illegal:231 Dlf:0\n",
      "Episode: 4000 Epsilon: 0.30 Reward: -16464 ElTime:187.84\n",
      "win:65 Lost:37 Tie:4 Illegal:394 Dlf:0\n",
      "Episode: 4500 Epsilon: 0.26 Reward: -13635 ElTime:210.73\n",
      "win:128 Lost:35 Tie:1 Illegal:336 Dlf:0\n",
      "Episode: 5000 Epsilon: 0.22 Reward: -9124 ElTime:220.11\n",
      "win:213 Lost:39 Tie:6 Illegal:242 Dlf:0\n",
      "Episode: 5500 Epsilon: 0.19 Reward: -8194 ElTime:219.97\n",
      "win:246 Lost:21 Tie:1 Illegal:232 Dlf:0\n",
      "Episode: 6000 Epsilon: 0.17 Reward: -7381 ElTime:216.73\n",
      "win:262 Lost:20 Tie:1 Illegal:217 Dlf:0\n",
      "Episode: 6500 Epsilon: 0.14 Reward: -5634 ElTime:224.62\n",
      "win:293 Lost:29 Tie:1 Illegal:177 Dlf:0\n",
      "Episode: 7000 Epsilon: 0.12 Reward: -5534 ElTime:218.98\n",
      "win:298 Lost:23 Tie:1 Illegal:178 Dlf:0\n",
      "Episode: 7500 Epsilon: 0.11 Reward: -3486 ElTime:217.51\n",
      "win:331 Lost:34 Tie:2 Illegal:133 Dlf:0\n",
      "Episode: 8000 Epsilon: 0.09 Reward: -3679 ElTime:221.07\n",
      "win:332 Lost:27 Tie:2 Illegal:139 Dlf:0\n",
      "Episode: 8500 Epsilon: 0.08 Reward: -2859 ElTime:223.14\n",
      "win:347 Lost:32 Tie:0 Illegal:121 Dlf:0\n",
      "Episode: 9000 Epsilon: 0.07 Reward: -2888 ElTime:220.67\n",
      "win:337 Lost:46 Tie:1 Illegal:116 Dlf:0\n",
      "Episode: 9500 Epsilon: 0.06 Reward: -2515 ElTime:220.07\n",
      "win:357 Lost:23 Tie:2 Illegal:118 Dlf:0\n",
      "Episode: 10000 Epsilon: 0.05 Reward: -1535 ElTime:237.04\n",
      "win:373 Lost:31 Tie:1 Illegal:95 Dlf:0\n",
      "Episode: 10500 Epsilon: 0.04 Reward: -1225 ElTime:226.95\n",
      "win:388 Lost:16 Tie:0 Illegal:96 Dlf:0\n",
      "Episode: 11000 Epsilon: 0.04 Reward: -556 ElTime:237.91\n",
      "win:390 Lost:35 Tie:1 Illegal:74 Dlf:0\n",
      "Episode: 11500 Epsilon: 0.03 Reward: 542 ElTime:238.37\n",
      "win:415 Lost:31 Tie:0 Illegal:54 Dlf:0\n",
      "Episode: 12000 Epsilon: 0.03 Reward: -555 ElTime:227.94\n",
      "win:393 Lost:29 Tie:1 Illegal:77 Dlf:0\n",
      "Episode: 12500 Epsilon: 0.02 Reward: 200 ElTime:223.15\n",
      "win:420 Lost:11 Tie:0 Illegal:69 Dlf:0\n",
      "Episode: 13000 Epsilon: 0.02 Reward: -478 ElTime:223.00\n",
      "win:405 Lost:13 Tie:0 Illegal:82 Dlf:0\n",
      "Episode: 13500 Epsilon: 0.02 Reward: 832 ElTime:224.12\n",
      "win:427 Lost:20 Tie:0 Illegal:53 Dlf:0\n",
      "Episode: 14000 Epsilon: 0.01 Reward: 398 ElTime:219.68\n",
      "win:417 Lost:22 Tie:0 Illegal:61 Dlf:0\n",
      "Episode: 14500 Epsilon: 0.01 Reward: 405 ElTime:224.89\n",
      "win:418 Lost:22 Tie:0 Illegal:60 Dlf:0\n"
     ]
    }
   ],
   "source": [
    "Episodes = 15000\n",
    "start_time = time.time()\n",
    "episode_time= time.time()\n",
    "agent_won = 0\n",
    "envn_won = 0\n",
    "tie_cnt = 0\n",
    "chk_prt_epsd = 500\n",
    "env = TicTacToe(9)\n",
    "agent = DQNAgent(9*3,9)\n",
    "#agent.model.load_weights('model12')\n",
    "score = 0\n",
    "status = 'Tie'\n",
    "ilcnt = 0\n",
    "wincnt = 0 \n",
    "locnt = 0\n",
    "ticnt = 0\n",
    "dfcnt = 0\n",
    "\n",
    "for episode in range(Episodes):\n",
    "    curr_state = []\n",
    "    curr_state = env.reset()\n",
    "    terminal_state = False\n",
    "    while not terminal_state:\n",
    "        state = curr_state.copy()\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward,terminal_state,status = env.step(state, action)\n",
    "        agent.append_sample(curr_state,action,reward,next_state,terminal_state)\n",
    "        curr_state = next_state\n",
    "        score += reward\n",
    "        agent.train_model()\n",
    "        \n",
    "    if status == 'Illegal':\n",
    "        ilcnt += 1;\n",
    "    elif status == 'Won':\n",
    "        wincnt += 1\n",
    "    elif status == 'Lose':\n",
    "        locnt += 1\n",
    "    elif status == 'Tie':\n",
    "        ticnt += 1\n",
    "    else:\n",
    "        dfcnt += 1\n",
    "    agent.update_epsilon(episode)\n",
    "    if (episode % chk_prt_epsd) == 0:\n",
    "        print(f'Episode: {episode} Epsilon: {agent.epsilon:.2f} Reward: {score} ElTime:{time.time() - episode_time:.2f}')\n",
    "        print(f'win:{wincnt} Lost:{locnt} Tie:{ticnt} Illegal:{ilcnt} Dlf:{dfcnt}')\n",
    "        score = 0\n",
    "        status = 'Tie'\n",
    "        ilcnt = 0\n",
    "        wincnt = 0 \n",
    "        locnt = 0\n",
    "        ticnt = 0\n",
    "        dfcnt = 0\n",
    "        episode_time = time.time()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T1hfqwSkBK1K"
   },
   "outputs": [],
   "source": [
    "agent.model.save_weights('model_weights/model12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jHrVHwKbBW59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc630312c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.load_weights('model_weights/model12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                896       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 2,249\n",
      "Trainable params: 2,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def print_state(state):\n",
    "    for i in range(0,9,3):\n",
    "        print(f\"\\n{state[i]}|{state[i+1]}|{state[i+2]}\")\n",
    "\n",
    "env = TicTacToe(9)\n",
    "agent = DQNAgent(9*3,9)\n",
    "agent.model.load_weights('model_weights/model12')\n",
    "print_state(env.state)\n",
    "print(agent.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "1. Agent played (4, 1) move\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|1|2\n",
      "\n",
      "0|0|0\n",
      "Env Reward:-1, Game Status:Resume\n",
      "\n",
      "2. Agent played (0, 1) move\n",
      "\n",
      "1|0|0\n",
      "\n",
      "0|1|2\n",
      "\n",
      "0|0|2\n",
      "Env Reward:-1, Game Status:Resume\n",
      "\n",
      "3. Agent played (2, 1) move\n",
      "\n",
      "1|0|1\n",
      "\n",
      "0|1|2\n",
      "\n",
      "0|2|2\n",
      "Env Reward:-1, Game Status:Resume\n",
      "\n",
      "4. Agent played (6, 1) move\n",
      "\n",
      "1|0|1\n",
      "\n",
      "0|1|2\n",
      "\n",
      "1|2|2\n",
      "Env Reward:10, Game Status:Won\n"
     ]
    }
   ],
   "source": [
    "terminal_state = False\n",
    "env.reset()\n",
    "print_state(env.state)\n",
    "cnt=1\n",
    "while(terminal_state != True):\n",
    "    action = agent.get_action(env.state,train=False)\n",
    "    print(f'\\n{cnt}. Agent played {action} move')\n",
    "    _,reward,terminal_state,status = env.step(env.state, action)\n",
    "    print_state(env.state)\n",
    "    print(f'Env Reward:{reward}, Game Status:{status}')\n",
    "    cnt +=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_Dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
