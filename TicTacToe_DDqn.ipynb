{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vhQOie0o4Ctn"
   },
   "outputs": [],
   "source": [
    "#numpy version = '1.20.2'\n",
    "#Keras version = '2.4.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nw3zP9NuqxF5"
   },
   "outputs": [],
   "source": [
    "# importing  TicTacToe class from environment file\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z3WgiQRlrCMq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The environment class for the Tic-Tac-Toe problem. Provides methods for initialising a state,\n",
    "checking whether the state is terminal or not,\n",
    "computing new states and rewards given state and action etc.\n",
    "'''\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self,grid_size):\n",
    "        \"\"\"initialise the board\"\"\"\n",
    "        # initialise state as an array\n",
    "        # initialises the board position, can initialise to an array or matrix\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "        # all possible numbers can initialise to an array or matrix\n",
    "    def is_winning(self, curr_state):\n",
    "        \"\"\"Takes state as an input and returns whether any row, column or diagonal has winning sum\"\"\"\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        for line in lines:\n",
    "            if np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]) and np.any(curr_state[line[0]]):\n",
    "                if (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==1):\n",
    "                    return True\n",
    "                elif (curr_state[line[0]]==curr_state[line[1]]==curr_state[line[2]]==2):\n",
    "                    return True\n",
    "        return False\n",
    "    def is_terminal(self, curr_state):\n",
    "        # Terminal state could be winning state or when the board is filled up\n",
    "        if self.is_winning(curr_state) == True:\n",
    "            return True, 'Won'\n",
    "        elif len(self.allowed_positions(curr_state)) ==0:\n",
    "            # all space got filled\n",
    "            return True, 'Tie'\n",
    "        else:\n",
    "            return False, 'Resume'\n",
    "    def allowed_positions(self,state):\n",
    "        # zero represent state available\n",
    "        return np.where(state == 0)[0]\n",
    "    def valid_actions(self, curr_state,player):\n",
    "        \"\"\"Takes the current state as input and returns all possible (unused) \n",
    "        values that can be placed on the board\"\"\"\n",
    "        values = []\n",
    "        unused_values = self.allowed_positions(curr_state)\n",
    "        if player == \"agent\":\n",
    "            values = [(val,1) for val in unused_values]\n",
    "        elif player ==\"Env\":\n",
    "            values = [(val,2) for val in unused_values]\n",
    "        return values\n",
    "    def state_transition(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the board position just after agent's move.\"\"\"\n",
    "        position = curr_action[0]\n",
    "        value = curr_action[1]\n",
    "        curr_state[position] = value \n",
    "        return curr_state\n",
    "    def step(self, curr_state, curr_action):\n",
    "        \"\"\"Takes current state and action and returns the next state and reward. \n",
    "        First, check the board position after agent's move, whether the game is won/loss/tied. \n",
    "        Then incorporate environment's move and again check the board status.\"\"\"\n",
    "        terminal_state = False\n",
    "        used_values = np.where(curr_state!=0)[0]\n",
    "        temp_state = self.state_transition(curr_state, curr_action)\n",
    "        if [val for val in used_values if val == curr_action[0]]:\n",
    "            # illegal move \n",
    "            reward = -40\n",
    "            terminal_state = True\n",
    "            game_status = 'Illegal'\n",
    "        else:\n",
    "            terminal_state, game_status = self.is_terminal(temp_state)\n",
    "            if terminal_state == True:\n",
    "                if game_status == 'Won':\n",
    "                    reward=10\n",
    "                else:\n",
    "                    reward=0\n",
    "            else:\n",
    "                # make random move from Environment\n",
    "                position,value = random.choice(self.valid_actions(temp_state,\"Env\"))\n",
    "                # postion= input()\n",
    "                # position,value = list(map(int,postion.split(','))) \n",
    "                # print(f'\\n    Env played {position,value} move')\n",
    "                temp_state[position]= value\n",
    "                terminal_state, game_status = self.is_terminal(temp_state)\n",
    "                if terminal_state == True:\n",
    "                    if game_status == 'Won':\n",
    "                        reward=-10\n",
    "                        game_status = 'Lose'\n",
    "                    else:\n",
    "                        reward=0\n",
    "                else:\n",
    "                    reward=-1\n",
    "        return temp_state, reward, terminal_state,game_status\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.grid_size,dtype=int)\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "8Af68Gl4rEDc",
    "outputId": "95493caf-7e4a-4174-98ec-f35025ce6c6e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPISODES = 10000\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "decay_rate = -0.0005       # epsilon decay rate\n",
    "epsilon = []\n",
    "episd = np.arange(0,EPISODES)\n",
    "for i in range(0,EPISODES):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(decay_rate*i))\n",
    "\n",
    "plt.plot(episd, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0UHx55RPrJGR"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01 \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon = 1\n",
    "        \n",
    "        self.batch_size = 64\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=512)\n",
    "        # create q network model and target model\n",
    "        self.model = self.build_model()\n",
    "        # create target model\n",
    "        self.targetmodel = self.build_model()\n",
    "        # update target model\n",
    "        self.updateTmodel()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    def encode(self,state):\n",
    "        en = en = np.zeros([9,3],dtype=int)\n",
    "        for i,l in enumerate(state):\n",
    "            en[i][l] = 1\n",
    "        return en.reshape(1,27)\n",
    "    \n",
    "    def updateTmodel(self):\n",
    "        # copy weights from model to targetmodel\n",
    "        self.targetmodel.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def update_epsilon(self,episode):\n",
    "        self.epsilon = (self.epsilon_max - self.epsilon_min) * np.exp(self.epsilon_decay * episode)\n",
    "\n",
    "    def get_action(self, state,train=True):\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in ε after we generate each sample from the environment\n",
    "        possible_actions = env.valid_actions(state,\"agent\")\n",
    "        if np.random.rand() <= self.epsilon and train==True:\n",
    "            return random.choice(possible_actions)\n",
    "        else :\n",
    "            q_value = self.model.predict(self.encode(state))\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            return (np.argmax(q_value),1)\n",
    "\n",
    "    def append_sample(self, curr_state, action, reward, next_state,done):\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        #print(curr_state, action, reward, next_state,done)\n",
    "        self.memory.append((curr_state, action, reward, next_state, done))\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_bool = mini_batch[i]\n",
    "                update_input[i] = self.encode(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_bool)\n",
    "                update_output[i] = self.encode(next_state)\n",
    "                \n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)    \n",
    "            #Get the s' from target model\n",
    "            target_qval = self.targetmodel.predict(update_output)\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                if not done[i]:\n",
    "                    # Only take the max q_value from valid actions from next state\n",
    "                    target[i][actions[i][0]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                else:\n",
    "                    target[i][actions[i][0]] = rewards[i]\n",
    "                \n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miXsU2ourc6h",
    "outputId": "e4536a02-bf60-4e02-d2e9-67fd2806c41c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Epsilon: 1.00 TotalReward: -4 ElpasedTime:0.76\n",
      "win:0 Lost:0 Tie:1 Illegal:0 Dlf:0\n",
      "Episode: 500 Epsilon: 0.78 TotalReward: -2394 ElpasedTime:229.23\n",
      "win:259 Lost:148 Tie:44 Illegal:49 Dlf:0\n",
      "Episode: 1000 Epsilon: 0.61 TotalReward: -3794 ElpasedTime:243.09\n",
      "win:256 Lost:93 Tie:54 Illegal:97 Dlf:0\n",
      "Episode: 1500 Epsilon: 0.47 TotalReward: -5100 ElpasedTime:245.10\n",
      "win:228 Lost:116 Tie:37 Illegal:119 Dlf:0\n",
      "Episode: 2000 Epsilon: 0.37 TotalReward: -6123 ElpasedTime:249.13\n",
      "win:226 Lost:82 Tie:39 Illegal:153 Dlf:0\n",
      "Episode: 2500 Epsilon: 0.29 TotalReward: -7333 ElpasedTime:256.54\n",
      "win:220 Lost:74 Tie:21 Illegal:185 Dlf:0\n",
      "Episode: 3000 Epsilon: 0.22 TotalReward: -5774 ElpasedTime:265.24\n",
      "win:254 Lost:68 Tie:22 Illegal:156 Dlf:0\n",
      "Episode: 3500 Epsilon: 0.17 TotalReward: -5306 ElpasedTime:263.43\n",
      "win:258 Lost:76 Tie:22 Illegal:144 Dlf:0\n",
      "Episode: 4000 Epsilon: 0.14 TotalReward: -3016 ElpasedTime:256.13\n",
      "win:320 Lost:53 Tie:19 Illegal:108 Dlf:0\n",
      "Episode: 4500 Epsilon: 0.11 TotalReward: -1744 ElpasedTime:256.44\n",
      "win:361 Lost:43 Tie:7 Illegal:89 Dlf:0\n",
      "Episode: 5000 Epsilon: 0.08 TotalReward: -2112 ElpasedTime:258.39\n",
      "win:354 Lost:38 Tie:9 Illegal:99 Dlf:0\n",
      "Episode: 5500 Epsilon: 0.06 TotalReward: -1444 ElpasedTime:263.27\n",
      "win:371 Lost:32 Tie:9 Illegal:88 Dlf:0\n",
      "Episode: 6000 Epsilon: 0.05 TotalReward: -2798 ElpasedTime:263.35\n",
      "win:348 Lost:28 Tie:7 Illegal:117 Dlf:0\n",
      "Episode: 6500 Epsilon: 0.04 TotalReward: -1751 ElpasedTime:262.85\n",
      "win:365 Lost:27 Tie:12 Illegal:96 Dlf:0\n",
      "Episode: 7000 Epsilon: 0.03 TotalReward: -2510 ElpasedTime:256.49\n",
      "win:339 Lost:47 Tie:10 Illegal:104 Dlf:0\n",
      "Episode: 7500 Epsilon: 0.02 TotalReward: -440 ElpasedTime:256.36\n",
      "win:381 Lost:43 Tie:12 Illegal:64 Dlf:0\n",
      "Episode: 8000 Epsilon: 0.02 TotalReward: 977 ElpasedTime:251.18\n",
      "win:414 Lost:41 Tie:7 Illegal:38 Dlf:0\n",
      "Episode: 8500 Epsilon: 0.01 TotalReward: 103 ElpasedTime:248.97\n",
      "win:397 Lost:37 Tie:9 Illegal:57 Dlf:0\n",
      "Episode: 9000 Epsilon: 0.01 TotalReward: 1837 ElpasedTime:254.10\n",
      "win:441 Lost:23 Tie:9 Illegal:27 Dlf:0\n",
      "Episode: 9500 Epsilon: 0.01 TotalReward: 1662 ElpasedTime:255.39\n",
      "win:438 Lost:22 Tie:9 Illegal:31 Dlf:0\n",
      "Total time consumed 255.75\n"
     ]
    }
   ],
   "source": [
    "Episodes = 10000\n",
    "start_time = time.time()\n",
    "episode_time= time.time()\n",
    "agent_won = 0\n",
    "envn_won = 0\n",
    "tie_cnt = 0\n",
    "chk_prt_epsd = 500\n",
    "env = TicTacToe(9)\n",
    "agent = DQNAgent(9*3,3*3)\n",
    "# agent.model.load_weights('model_ddqn/modelDDqn')\n",
    "# agent.updateTmodel()\n",
    "score = 0\n",
    "status = 'Tie'\n",
    "ilcnt = 0\n",
    "wincnt = 0 \n",
    "locnt = 0\n",
    "ticnt = 0\n",
    "dfcnt = 0\n",
    "\n",
    "for episode in range(Episodes):\n",
    "    curr_state = []\n",
    "    curr_state = env.reset()\n",
    "    terminal_state = False\n",
    "    while not terminal_state:\n",
    "        state = curr_state.copy()\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward,terminal_state,status = env.step(state, action)\n",
    "        agent.append_sample(curr_state,action,reward,next_state,terminal_state)\n",
    "        curr_state = next_state\n",
    "        score += reward\n",
    "        agent.train_model()\n",
    "        \n",
    "    if status == 'Illegal':\n",
    "        ilcnt += 1;\n",
    "    elif status == 'Won':\n",
    "        wincnt += 1\n",
    "    elif status == 'Lose':\n",
    "        locnt += 1\n",
    "    elif status == 'Tie':\n",
    "        ticnt += 1\n",
    "    else:\n",
    "        dfcnt += 1\n",
    "    agent.updateTmodel()\n",
    "    agent.update_epsilon(episode)\n",
    "    if (episode % chk_prt_epsd) == 0:\n",
    "        print(f'Episode: {episode} Epsilon: {agent.epsilon:.2f} TotalReward: {score} ElpasedTime:{time.time() - episode_time:.2f}')\n",
    "        print(f'win:{wincnt} Lost:{locnt} Tie:{ticnt} Illegal:{ilcnt} Dlf:{dfcnt}')\n",
    "        score = 0\n",
    "        status = 'Tie'\n",
    "        ilcnt = 0\n",
    "        wincnt = 0 \n",
    "        locnt = 0\n",
    "        ticnt = 0\n",
    "        dfcnt = 0\n",
    "        episode_time = time.time()\n",
    "print(f'Total time consumed {time.time() - episode_time:.2f}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T1hfqwSkBK1K"
   },
   "outputs": [],
   "source": [
    "agent.model.save_weights('model_ddqn/modelDDqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHrVHwKbBW59",
    "outputId": "b012dfe9-0d6a-4eac-c0ea-d4e60f72fb6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd2bbe72d10>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.load_weights('model_ddqn/modelDDqn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIIRrvr9gIVe",
    "outputId": "22e4909f-ecd8-4c55-d183-b60b65b8b4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                896       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 2,249\n",
      "Trainable params: 2,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "def print_state(state):\n",
    "    for i in range(0,9,3):\n",
    "        print(f\"\\n{state[i]}|{state[i+1]}|{state[i+2]}\")\n",
    "\n",
    "env = TicTacToe(9)\n",
    "agent = DQNAgent(9*3,9)\n",
    "agent.model.load_weights('model_ddqn/modelDDqn')\n",
    "print_state(env.state)\n",
    "print(agent.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOL1SXnTgS15",
    "outputId": "1478f641-15a3-4795-e249-0f8a9dd2dc5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "1. Agent played (0, 1) move\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "\n",
      "0|0|0\n",
      "4,2\n",
      "\n",
      "    Env played (4, 2) move\n",
      "Env Reward:-1, Game Status:Resume\n",
      "\n",
      "2. Agent played (2, 1) move\n",
      "\n",
      "1|0|0\n",
      "\n",
      "0|2|0\n",
      "\n",
      "0|0|0\n",
      "6,2\n",
      "\n",
      "    Env played (6, 2) move\n",
      "Env Reward:-1, Game Status:Resume\n",
      "\n",
      "3. Agent played (1, 1) move\n",
      "\n",
      "1|0|1\n",
      "\n",
      "0|2|0\n",
      "\n",
      "2|0|0\n",
      "Env Reward:10, Game Status:Won\n"
     ]
    }
   ],
   "source": [
    "terminal_state = False\n",
    "env.reset()\n",
    "print_state(env.state)\n",
    "cnt=1\n",
    "while(terminal_state != True):\n",
    "    action = agent.get_action(env.state,train=False)\n",
    "    print(f'\\n{cnt}. Agent played {action} move')\n",
    "    print_state(env.state)\n",
    "    _,reward,terminal_state,status = env.step(env.state, action)\n",
    "    print(f'Env Reward:{reward}, Game Status:{status}')\n",
    "    cnt +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVYs8Zh0hTzA",
    "outputId": "098e2aa2-6775-4026-9c25-c0ecb14f3589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1|1|1\n",
      "\n",
      "0|2|0\n",
      "\n",
      "2|0|0\n"
     ]
    }
   ],
   "source": [
    "print_state(env.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1E7SqPShVPO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_DDqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
